{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68250774-8548-43f9-ba49-8df7acd6ad30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are: spasov\n",
      "credentials validity: 145 hours left.\n",
      "shared namespace is: iceberg.com490_iceberg\n",
      "your namespace is: iceberg.spasov\n",
      "your group is: U1\n",
      "Warehouse URL: https://iccluster028.iccluster.epfl.ch:8443/\n",
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "import base64 as b64\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"pandas only supports SQLAlchemy connectable .*\")\n",
    "\n",
    "import pwd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from random import randrange\n",
    "import pyspark.sql.functions as F\n",
    "#np.bool = np.bool_\n",
    "\n",
    "import trino\n",
    "from contextlib import closing\n",
    "from urllib.parse import urlparse\n",
    "from trino.dbapi import connect\n",
    "from trino.auth import BasicAuthentication, JWTAuthentication\n",
    "\n",
    "groupName = 'U1'\n",
    "\n",
    "def getUsername():\n",
    "    payload = os.environ.get('EPFL_COM490_TOKEN').split('.')[1]\n",
    "    payload=payload+'=' * (4 - len(payload) % 4)\n",
    "    obj = json.loads(b64.urlsafe_b64decode(payload))\n",
    "    if (time.time() > int(obj.get('exp')) - 3600):\n",
    "        raise Exception('Your credentials have expired, please restart your Jupyter Hub server:'\n",
    "                        'File>Hub Control Panel, Stop My Server, Start My Server.')\n",
    "    time_left = int((obj.get('exp') - time.time())/3600)\n",
    "    return obj.get('sub'), time_left\n",
    "\n",
    "username, validity_h = getUsername()\n",
    "hadoopFS = os.environ.get('HADOOP_FS')\n",
    "namespace = 'iceberg.' + username\n",
    "sharedNS = 'iceberg.com490_iceberg'\n",
    "\n",
    "if not re.search('[A-Z][0-9]', groupName):\n",
    "    raise Exception('Invalid group name {groupName}')\n",
    "\n",
    "print(f\"you are: {username}\")\n",
    "print(f\"credentials validity: {validity_h} hours left.\")\n",
    "print(f\"shared namespace is: {sharedNS}\")\n",
    "print(f\"your namespace is: {namespace}\")\n",
    "print(f\"your group is: {groupName}\")\n",
    "\n",
    "trinoAuth = JWTAuthentication(os.environ.get('EPFL_COM490_TOKEN'))\n",
    "trinoUrl  = urlparse(os.environ.get('TRINO_URL'))\n",
    "Query=[]\n",
    "\n",
    "print(f\"Warehouse URL: {trinoUrl.scheme}://{trinoUrl.hostname}:{trinoUrl.port}/\")\n",
    "\n",
    "conn = connect(\n",
    "    host=trinoUrl.hostname,\n",
    "    port=trinoUrl.port,\n",
    "    auth=trinoAuth,\n",
    "    http_scheme=trinoUrl.scheme,\n",
    "    verify=True\n",
    ")\n",
    "\n",
    "print('Connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaba822e-0ca5-4c87-baf5-0522dc7f3628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\n",
      "hadoopFSs=hdfs://iccluster059.iccluster.epfl.ch:9000\n",
      "username=spasov\n",
      "group=U1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/25 16:33:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "username = pwd.getpwuid(os.getuid()).pw_name\n",
    "hadoopFS=os.getenv('HADOOP_FS', None)\n",
    "\n",
    "print(os.getenv('SPARK_HOME'))\n",
    "print(f\"hadoopFSs={hadoopFS}\")\n",
    "print(f\"username={username}\")\n",
    "print(f\"group={groupName}\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(pwd.getpwuid(os.getuid()).pw_name)\\\n",
    "            .config('spark.ui.port', randrange(4040, 4440, 5))\\\n",
    "            .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "            .config('spark.jars', f'{hadoopFS}/data/com-490/jars/iceberg-spark-runtime-3.5_2.13-1.6.1.jar')\\\n",
    "            .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    "            .config('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog')\\\n",
    "            .config('spark.sql.catalog.iceberg.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.iceberg.warehouse', f'{hadoopFS}/data/com-490/iceberg/')\\\n",
    "            .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.warehouse', f'{hadoopFS}/user/{username}/assignment-3/warehouse')\\\n",
    "            .config(\"spark.sql.warehouse.dir\", f'{hadoopFS}/user/{username}/assignment-3/spark/warehouse')\\\n",
    "            .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'G1 Young Generation')\\\n",
    "            .config(\"spark.executor.memory\", \"6g\")\\\n",
    "            .config(\"spark.executor.cores\", \"4\")\\\n",
    "            .config(\"spark.executor.instances\", \"4\")\\\n",
    "            .master('yarn')\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb87e3c-89d5-43e6-b5ad-11d0059cb9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://iccluster087.iccluster.epfl.ch:4395\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spasov</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=spasov>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5635f1f9-358a-4d76-a290-bd18e569f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- arr_delay_sec: double (nullable = true)\n",
      " |-- dep_hour: integer (nullable = true)\n",
      " |-- total_daily_precip: double (nullable = true)\n",
      " |-- is_raining: integer (nullable = true)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      " |-- dow_str: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- dow_idx: double (nullable = true)\n",
      " |-- dow_vec: vector (nullable = true)\n",
      " |-- type_idx: double (nullable = true)\n",
      " |-- type_vec: vector (nullable = true)\n",
      " |-- stopname_idx: double (nullable = true)\n",
      " |-- stopname_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regression_df = spark.read.parquet(f\"{hadoopFS}/user/com-490/group/U1/predictions_regression.parquet\")\n",
    "regression_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
