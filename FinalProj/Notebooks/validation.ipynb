{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e96e9497-0b3c-43e6-bf92-ada74a8f3954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported successfully!\n",
      "/opt/spark\n",
      "hadoopFSs=hdfs://iccluster059.iccluster.epfl.ch:9000\n",
      "username=omanovic\n",
      "group=U1\n",
      "you are: omanovic\n",
      "credentials validity: 160 hours left.\n",
      "shared namespace is: iceberg.com490_iceberg\n",
      "your namespace is: iceberg.omanovic\n",
      "your group is: U1\n",
      "Warehouse URL: https://iccluster028.iccluster.epfl.ch:8443/\n",
      "Connected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 22:07:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import importlib.util\n",
    "import os\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "# Load preprocessing module\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", '../scripts/preprocessing.py')\n",
    "preprocess = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(preprocess)\n",
    "print(preprocess.test())\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import pwd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import base64 as b64\n",
    "from contextlib import closing\n",
    "from urllib.parse import urlparse\n",
    "from random import randrange\n",
    "from itertools import chain\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import trino\n",
    "from trino.dbapi import connect\n",
    "from trino.auth import BasicAuthentication, JWTAuthentication\n",
    "import seaborn as sns\n",
    "\n",
    "# PySpark core\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, IntegerType, DateType\n",
    "\n",
    "# PySpark SQL functions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import rank, col, avg, date_format, count, year,expr, coalesce, lit, to_timestamp,unix_timestamp, hour, month, when,create_map, monotonically_increasing_id\n",
    "\n",
    "# PySpark ML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler, StandardScaler,IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression,GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "#setup spark session and trino\n",
    "username = pwd.getpwuid(os.getuid()).pw_name\n",
    "hadoopFS=os.getenv('HADOOP_FS', None)\n",
    "groupName = 'U1'\n",
    "print(os.getenv('SPARK_HOME'))\n",
    "print(f\"hadoopFSs={hadoopFS}\")\n",
    "print(f\"username={username}\")\n",
    "print(f\"group={groupName}\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(pwd.getpwuid(os.getuid()).pw_name)\\\n",
    "            .config('spark.ui.port', randrange(4040, 4440, 5))\\\n",
    "            .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "            .config('spark.jars', f'{hadoopFS}/data/com-490/jars/iceberg-spark-runtime-3.5_2.13-1.6.1.jar')\\\n",
    "            .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    "            .config('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog')\\\n",
    "            .config('spark.sql.catalog.iceberg.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.iceberg.warehouse', f'{hadoopFS}/data/com-490/iceberg/')\\\n",
    "            .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.warehouse', f'{hadoopFS}/user/{username}/assignment-3/warehouse')\\\n",
    "            .config(\"spark.sql.warehouse.dir\", f'{hadoopFS}/user/{username}/assignment-3/spark/warehouse')\\\n",
    "            .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'G1 Young Generation')\\\n",
    "            .config(\"spark.executor.memory\", \"6g\")\\\n",
    "            .config(\"spark.executor.cores\", \"4\")\\\n",
    "            .config(\"spark.executor.instances\", \"4\")\\\n",
    "            .master('yarn')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"pandas only supports SQLAlchemy connectable .*\")\n",
    "\n",
    "\n",
    "\n",
    "def getUsername():\n",
    "    payload = os.environ.get('EPFL_COM490_TOKEN').split('.')[1]\n",
    "    payload=payload+'=' * (4 - len(payload) % 4)\n",
    "    obj = json.loads(b64.urlsafe_b64decode(payload))\n",
    "    if (time.time() > int(obj.get('exp')) - 3600):\n",
    "        raise Exception('Your credentials have expired, please restart your Jupyter Hub server:'\n",
    "                        'File>Hub Control Panel, Stop My Server, Start My Server.')\n",
    "    time_left = int((obj.get('exp') - time.time())/3600)\n",
    "    return obj.get('sub'), time_left\n",
    "\n",
    "username, validity_h = getUsername()\n",
    "hadoopFS = os.environ.get('HADOOP_FS')\n",
    "namespace = 'iceberg.' + username\n",
    "sharedNS = 'iceberg.com490_iceberg'\n",
    "\n",
    "if not re.search('[A-Z][0-9]', groupName):\n",
    "    raise Exception('Invalid group name {groupName}')\n",
    "\n",
    "print(f\"you are: {username}\")\n",
    "print(f\"credentials validity: {validity_h} hours left.\")\n",
    "print(f\"shared namespace is: {sharedNS}\")\n",
    "print(f\"your namespace is: {namespace}\")\n",
    "print(f\"your group is: {groupName}\")\n",
    "\n",
    "trinoAuth = JWTAuthentication(os.environ.get('EPFL_COM490_TOKEN'))\n",
    "trinoUrl  = urlparse(os.environ.get('TRINO_URL'))\n",
    "Query=[]\n",
    "\n",
    "print(f\"Warehouse URL: {trinoUrl.scheme}://{trinoUrl.hostname}:{trinoUrl.port}/\")\n",
    "\n",
    "conn = connect(\n",
    "    host=trinoUrl.hostname,\n",
    "    port=trinoUrl.port,\n",
    "    auth=trinoAuth,\n",
    "    http_scheme=trinoUrl.scheme,\n",
    "    verify=True\n",
    ")\n",
    "\n",
    "print('Connected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7baa80-1edf-4287-ab85-6dcadd9e7fd7",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fe450f-df90-4c38-ba3a-af18e4913a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_preds = spark.read.parquet(f\"{hadoopFS}/user/com-490/group/U1/multiclass_model.parquet\") # previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76796a0-e781-48d3-81f0-f77b3b73f075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_preds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66aab1-3d77-46e1-a7c8-5dfa0c87ea99",
   "metadata": {},
   "source": [
    "# Class-wise precision - convert to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb4b17d-89b8-4548-bc56-5608b8091e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 22:08:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class     Precision  Recall  F1-score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small         0.928   0.724      0.813\n",
      "medium        0.161   0.525      0.247\n",
      "big           0.130   0.022      0.037\n",
      "\n",
      "Weighted Precision: 0.840\n",
      "Weighted Recall:    0.692\n",
      "Weighted F1-score:  0.759\n",
      "Overall Accuracy:   0.692\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import create_map, lit, col\n",
    "from itertools import chain\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DelayCategoryMetrics\").getOrCreate()\n",
    "\n",
    "label_list = [\"small\", \"medium\", \"big\"]\n",
    "entries = list(chain.from_iterable([(lbl, float(idx)) \n",
    "                                    for idx, lbl in enumerate(label_list)]))\n",
    "label_map = create_map(*[lit(x) for x in entries])\n",
    "\n",
    "eval_df = full_preds \\\n",
    "    .withColumn(\"label_idx\", label_map[col(\"delay_category\")]) \\\n",
    "    .withColumn(\"pred_idx\",  label_map[col(\"predicted_delay_cat\")])\n",
    "\n",
    "pred_label_rdd = eval_df.select(\"pred_idx\", \"label_idx\") \\\n",
    "    .rdd.map(lambda row: (row.pred_idx, row.label_idx))\n",
    "\n",
    "metrics = MulticlassMetrics(pred_label_rdd)\n",
    "\n",
    "print(\"Class     Precision  Recall  F1-score\")\n",
    "for idx, lbl in enumerate(label_list):\n",
    "    p = metrics.precision(float(idx))\n",
    "    r = metrics.recall(float(idx))\n",
    "    f1 = 2*p*r/(p+r) if (p + r) > 0 else 0.0\n",
    "    print(f\"{lbl:6s}    {p:9.3f}  {r:6.3f}   {f1:8.3f}\")\n",
    "\n",
    "wp = metrics.weightedPrecision\n",
    "wr = metrics.weightedRecall\n",
    "wf1 = 2*wp*wr/(wp+wr) if (wp + wr) > 0 else 0.0\n",
    "acc = metrics.accuracy\n",
    "\n",
    "print(\"\\nWeighted Precision: {:.3f}\".format(wp))\n",
    "print(\"Weighted Recall:    {:.3f}\".format(wr))\n",
    "print(\"Weighted F1-score:  {:.3f}\".format(wf1))\n",
    "print(\"Overall Accuracy:   {:.3f}\".format(acc))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae72d7-fe56-40fb-a6f7-2d7bbbbad29d",
   "metadata": {},
   "source": [
    "# Metrics overall:\n",
    "\n",
    "```\n",
    "Class     Precision  Recall  F1-score\n",
    "                                                                                \n",
    "small         0.928   0.724      0.813\n",
    "\n",
    "medium        0.161   0.525      0.247\n",
    "\n",
    "big           0.130   0.022      0.037\n",
    "\n",
    "Weighted Precision: 0.840\n",
    "Weighted Recall:    0.692\n",
    "Weighted F1-score:  0.759\n",
    "Overall Accuracy:   0.692\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e35cca-f233-4b4b-a2b8-6971268d44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "label_to_idx = {lbl: i for i,lbl in enumerate(label_ix.labels)}\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=None,  \n",
    "    labelCol=\"label\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "aucs = {}\n",
    "for cls in [\"small\",\"medium\",\"big\"]:\n",
    "    idx = label_to_idx[cls]\n",
    "    prob_col = f\"prob_{cls}\"\n",
    "    one_vs_rest = full_preds.withColumn(\n",
    "        \"isPos\",\n",
    "        when(col(\"label\") == idx, 1.0).otherwise(0.0)\n",
    "    )\n",
    "    evaluator.setRawPredictionCol(prob_col).setLabelCol(\"isPos\")\n",
    "    auc = evaluator.evaluate(one_vs_rest)\n",
    "    aucs[cls] = auc\n",
    "\n",
    "print(\"One-vs-rest AUCs:\", aucs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c3463-c954-4bb8-918b-31c2adc17833",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = full_preds.select(\"delay_category_q\",\"predicted_category_q\").toPandas()\n",
    "cm  = pd.crosstab(pdf.delay_category_q, pdf.predicted_category_q,rownames=['True'], colnames=['Pred'], normalize='index')\n",
    "\n",
    "plt.figure(figsize=(8,8),dpi=1200)\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", vmin=0, vmax=1)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel('True Category')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
