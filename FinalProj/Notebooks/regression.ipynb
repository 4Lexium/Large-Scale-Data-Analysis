{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad19a4af-c223-4795-9adb-bbb6e6587383",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported successfully!\n",
      "/opt/spark\n",
      "hadoopFSs=hdfs://iccluster059.iccluster.epfl.ch:9000\n",
      "username=omanovic\n",
      "group=U1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/24 17:56:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are: omanovic\n",
      "credentials validity: 167 hours left.\n",
      "shared namespace is: iceberg.com490_iceberg\n",
      "your namespace is: iceberg.omanovic\n",
      "your group is: U1\n",
      "Warehouse URL: https://iccluster028.iccluster.epfl.ch:8443/\n",
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import os\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "# Load preprocessing module\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", '../scripts/preprocessing.py')\n",
    "preprocess = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(preprocess)\n",
    "print(preprocess.test())\n",
    "#imports\n",
    "import os\n",
    "import pwd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from random import randrange\n",
    "import pyspark.sql.functions as F\n",
    "#np.bool = np.bool_\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler)\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.types import DoubleType, IntegerType, DateType\n",
    "from pyspark.sql.functions import rank, col, avg, date_format, count, year, expr, coalesce, lit, to_timestamp, unix_timestamp, hour,when\n",
    "from sklearn.ensemble        import GradientBoostingRegressor\n",
    "from sklearn.metrics         import mean_squared_error, r2_score\n",
    "import base64 as b64\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "import trino\n",
    "from contextlib import closing\n",
    "from urllib.parse import urlparse\n",
    "from trino.dbapi import connect\n",
    "from trino.auth import BasicAuthentication, JWTAuthentication\n",
    "#setup spark session and trino\n",
    "username = pwd.getpwuid(os.getuid()).pw_name\n",
    "hadoopFS=os.getenv('HADOOP_FS', None)\n",
    "groupName = 'U1'\n",
    "print(os.getenv('SPARK_HOME'))\n",
    "print(f\"hadoopFSs={hadoopFS}\")\n",
    "print(f\"username={username}\")\n",
    "print(f\"group={groupName}\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(pwd.getpwuid(os.getuid()).pw_name)\\\n",
    "            .config('spark.ui.port', randrange(4040, 4440, 5))\\\n",
    "            .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "            .config('spark.jars', f'{hadoopFS}/data/com-490/jars/iceberg-spark-runtime-3.5_2.13-1.6.1.jar')\\\n",
    "            .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    "            .config('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog')\\\n",
    "            .config('spark.sql.catalog.iceberg.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.iceberg.warehouse', f'{hadoopFS}/data/com-490/iceberg/')\\\n",
    "            .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.warehouse', f'{hadoopFS}/user/{username}/assignment-3/warehouse')\\\n",
    "            .config(\"spark.sql.warehouse.dir\", f'{hadoopFS}/user/{username}/assignment-3/spark/warehouse')\\\n",
    "            .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'G1 Young Generation')\\\n",
    "            .config(\"spark.executor.memory\", \"6g\")\\\n",
    "            .config(\"spark.executor.cores\", \"4\")\\\n",
    "            .config(\"spark.executor.instances\", \"4\")\\\n",
    "            .master('yarn')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"pandas only supports SQLAlchemy connectable .*\")\n",
    "\n",
    "\n",
    "\n",
    "def getUsername():\n",
    "    payload = os.environ.get('EPFL_COM490_TOKEN').split('.')[1]\n",
    "    payload=payload+'=' * (4 - len(payload) % 4)\n",
    "    obj = json.loads(b64.urlsafe_b64decode(payload))\n",
    "    if (time.time() > int(obj.get('exp')) - 3600):\n",
    "        raise Exception('Your credentials have expired, please restart your Jupyter Hub server:'\n",
    "                        'File>Hub Control Panel, Stop My Server, Start My Server.')\n",
    "    time_left = int((obj.get('exp') - time.time())/3600)\n",
    "    return obj.get('sub'), time_left\n",
    "\n",
    "username, validity_h = getUsername()\n",
    "hadoopFS = os.environ.get('HADOOP_FS')\n",
    "namespace = 'iceberg.' + username\n",
    "sharedNS = 'iceberg.com490_iceberg'\n",
    "\n",
    "if not re.search('[A-Z][0-9]', groupName):\n",
    "    raise Exception('Invalid group name {groupName}')\n",
    "\n",
    "print(f\"you are: {username}\")\n",
    "print(f\"credentials validity: {validity_h} hours left.\")\n",
    "print(f\"shared namespace is: {sharedNS}\")\n",
    "print(f\"your namespace is: {namespace}\")\n",
    "print(f\"your group is: {groupName}\")\n",
    "\n",
    "trinoAuth = JWTAuthentication(os.environ.get('EPFL_COM490_TOKEN'))\n",
    "trinoUrl  = urlparse(os.environ.get('TRINO_URL'))\n",
    "Query=[]\n",
    "\n",
    "print(f\"Warehouse URL: {trinoUrl.scheme}://{trinoUrl.hostname}:{trinoUrl.port}/\")\n",
    "\n",
    "conn = connect(\n",
    "    host=trinoUrl.hostname,\n",
    "    port=trinoUrl.port,\n",
    "    auth=trinoAuth,\n",
    "    http_scheme=trinoUrl.scheme,\n",
    "    verify=True\n",
    ")\n",
    "\n",
    "print('Connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de65bde7-a987-438b-a30b-71b0fc64e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.read.parquet(f\"{hadoopFS}/user/com-490/group/U1/raw_df_modelling.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e2e26cf-3daf-43f8-89e6-ca2a57d835af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- operating_day: string (nullable = true)\n",
      " |-- dow: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_actual: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_actual: string (nullable = true)\n",
      " |-- arr_delay_sec: string (nullable = true)\n",
      " |-- dep_delay_sec: string (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- total_daily_precip: string (nullable = true)\n",
      " |-- rained: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- stop_lat: string (nullable = true)\n",
      " |-- stop_lon: string (nullable = true)\n",
      "\n",
      "+-------+-------------+---+-----------+-----+--------------------+--------------------+--------------------+--------------------+-------------+-------------+----+------------------+------+---------------+--------------------+--------------------+\n",
      "|stop_id|operating_day|dow|    trip_id| type|            arr_time|          arr_actual|            dep_time|          dep_actual|arr_delay_sec|dep_delay_sec|site|total_daily_precip|rained|      stop_name|            stop_lat|            stop_lon|\n",
      "+-------+-------------+---+-----------+-----+--------------------+--------------------+--------------------+--------------------+-------------+-------------+----+------------------+------+---------------+--------------------+--------------------+\n",
      "|8592133|   2024-12-11|  4|85:151:6479|Metro|2024-12-11T07:55:...|2024-12-11T07:57:...|2024-12-11T07:55:...|2024-12-11T07:58:...|          160|          194|LSGL|               0.0| false|Lausanne, Vigie|46.52153425000000...|6.623925380000000000|\n",
      "+-------+-------------+---+-----------+-----+--------------------+--------------------+--------------------+--------------------+-------------+-------------+----+------------------+------+---------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>                                                       (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "raw.printSchema()\n",
    "raw.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "494c0bb9-d805-4e72-b6ca-7e8cb0b23bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>               (0 + 1) / 1][Stage 2300:>               (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------------------------+-------------+------------------+\n",
      "|trip_id        |stop_id|stop_name                 |arr_delay_sec|prediction        |\n",
      "+---------------+-------+--------------------------+-------------+------------------+\n",
      "|85:11:18416:002|8501120|Lausanne                  |17.0         |12.403536300524877|\n",
      "|85:11:24462:001|8501120|Lausanne                  |15.0         |38.68613081535594 |\n",
      "|85:11:2511:003 |8501120|Lausanne                  |-56.0        |38.5291510476241  |\n",
      "|85:151:1001    |8592053|Lausanne, Grande-Borde    |-75.0        |88.33627450208508 |\n",
      "|85:151:1006    |8592102|Lausanne, Prélaz          |36.0         |58.94811914538086 |\n",
      "|85:151:101     |8592005|Lausanne, Cèdres          |-41.0        |52.21547490135334 |\n",
      "|85:151:1020    |8592001|Lausanne, Boston          |-71.0        |84.76855877864236 |\n",
      "|85:151:1035    |8592103|Lausanne, Prélaz-les-Roses|232.0        |75.19760128231327 |\n",
      "|85:151:1052    |8592001|Lausanne, Boston          |199.0        |83.59039641940437 |\n",
      "|85:151:1055    |8592121|Lausanne, Théâtre de Vidy |-118.0       |78.3584112596151  |\n",
      "+---------------+-------+--------------------------+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types    import DoubleType, IntegerType, DateType, TimestampType\n",
    "from pyspark.sql.functions import (\n",
    "    to_timestamp, unix_timestamp, col,\n",
    "    hour, when, date_format\n",
    ")\n",
    "from pyspark.ml            import Pipeline\n",
    "from pyspark.ml.feature    import (\n",
    "    StringIndexer, OneHotEncoder,\n",
    "    VectorAssembler\n",
    ")\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# 2) Parse timestamps & recompute arr_delay_sec\n",
    "df = raw \\\n",
    "  .withColumn(\"arr_time_ts\",   to_timestamp(\"arr_time\")) \\\n",
    "  .withColumn(\"arr_actual_ts\", to_timestamp(\"arr_actual\")) \\\n",
    "  .withColumn(\"dep_time_ts\",   to_timestamp(\"dep_time\")) \\\n",
    "  .withColumn(\"dep_actual_ts\", to_timestamp(\"dep_actual\")) \\\n",
    "  .withColumn(\"arr_delay_sec\",\n",
    "      ( unix_timestamp(\"arr_actual_ts\") - unix_timestamp(\"arr_time_ts\") )\n",
    "        .cast(DoubleType())\n",
    "   ) \\\n",
    "  .filter(col(\"arr_delay_sec\").between(-300, 600))\n",
    "\n",
    "# 3) Cast numeric strings to doubles/ints\n",
    "features = df \\\n",
    "  .withColumn(\"total_daily_precip\", col(\"total_daily_precip\").cast(DoubleType())) \\\n",
    "  .withColumn(\"stop_lat\",           col(\"stop_lat\").cast(DoubleType())) \\\n",
    "  .withColumn(\"stop_lon\",           col(\"stop_lon\").cast(DoubleType())) \\\n",
    "  .withColumn(\"dep_hour\",           hour(\"dep_time_ts\").cast(IntegerType()))\n",
    "\n",
    "# 4) Build simple engineered features\n",
    "features = features \\\n",
    "  .withColumn(\"is_raining\", when(col(\"total_daily_precip\") > 0, 1).otherwise(0)) \\\n",
    "  .withColumn(\"dow_str\",    date_format(col(\"operating_day\").cast(DateType()), \"E\"))\n",
    "\n",
    "# 5) Select the columns you need and sample a small subset for debugging (1%)\n",
    "sampled = features.select(\n",
    "    \"trip_id\",\"stop_id\",\"arr_delay_sec\",\n",
    "    \"dep_hour\",\"total_daily_precip\",\"is_raining\",\n",
    "    \"stop_lat\",\"stop_lon\",\n",
    "    \"dow_str\",\"type\",\"stop_name\"\n",
    ").sample(False, 0.01, seed=42)\n",
    "\n",
    "# 6) Index & one‐hot encode all categoricals\n",
    "dow_idx      = StringIndexer(inputCol=\"dow_str\",    outputCol=\"dow_idx\",      handleInvalid=\"keep\")\n",
    "dow_ohe      = OneHotEncoder(inputCol=\"dow_idx\",     outputCol=\"dow_vec\")\n",
    "type_idx     = StringIndexer(inputCol=\"type\",       outputCol=\"type_idx\",     handleInvalid=\"keep\")\n",
    "type_ohe     = OneHotEncoder(inputCol=\"type_idx\",    outputCol=\"type_vec\")\n",
    "stopname_idx = StringIndexer(inputCol=\"stop_name\",  outputCol=\"stopname_idx\", handleInvalid=\"keep\")\n",
    "stopname_ohe = OneHotEncoder(inputCol=\"stopname_idx\",outputCol=\"stopname_vec\")\n",
    "\n",
    "# 7) Assemble everything into “features”\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "      \"dep_hour\",\"total_daily_precip\",\"is_raining\",\n",
    "      \"stop_lat\",\"stop_lon\",\n",
    "      \"dow_vec\",\"type_vec\",\"stopname_vec\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# 8) Pick a lighter regressor for debug (e.g. small GBT)\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"arr_delay_sec\",\n",
    "    maxDepth=5,\n",
    "    maxIter=20,\n",
    "    stepSize=0.2\n",
    ")\n",
    "\n",
    "# 9) Build the pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    dow_idx, dow_ohe,\n",
    "    type_idx, type_ohe,\n",
    "    stopname_idx, stopname_ohe,\n",
    "    assembler,\n",
    "    gbt\n",
    "])\n",
    "\n",
    "# 10) Train/test split on the sampled data\n",
    "train, test = sampled.randomSplit([0.8,0.2], seed=42)\n",
    "\n",
    "# 11) Fit & predict (should finish in seconds)\n",
    "model = pipeline.fit(train)\n",
    "pred  = model.transform(test)\n",
    "\n",
    "# 12) Inspect a few results\n",
    "pred.select(\"trip_id\",\"stop_id\",\"stop_name\",\"arr_delay_sec\",\"prediction\") \\\n",
    "    .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d5d3ab5-afee-4a13-a66f-2b0bf0ffede4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>               (0 + 1) / 1][Stage 2305:===========>  (23 + 6) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 129.67 s\n",
      "MAE  = 95.36 s\n",
      "R²   = 0.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>                                                       (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# 8) EVALUATE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"arr_delay_sec\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(pred, {evaluator.metricName: \"rmse\"})\n",
    "mae  = evaluator.evaluate(pred, {evaluator.metricName: \"mae\"})\n",
    "r2   = evaluator.evaluate(pred, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"RMSE = {rmse:.2f} s\")\n",
    "print(f\"MAE  = {mae:.2f} s\")\n",
    "print(f\"R²   = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7af5e1d-2ed8-46d1-bdaa-b3d76060520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>                                                       (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "pred.write.parquet(f\"{hadoopFS}/user/com-490/group/U1/predictions_regression.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee5cecf2-2499-40df-bac7-e733c8aef9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- arr_delay_sec: double (nullable = true)\n",
      " |-- dep_hour: integer (nullable = true)\n",
      " |-- total_daily_precip: double (nullable = true)\n",
      " |-- is_raining: integer (nullable = false)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      " |-- dow_str: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- dow_idx: double (nullable = false)\n",
      " |-- dow_vec: vector (nullable = true)\n",
      " |-- type_idx: double (nullable = false)\n",
      " |-- type_vec: vector (nullable = true)\n",
      " |-- stopname_idx: double (nullable = false)\n",
      " |-- stopname_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>                                                       (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7415e60-5663-41ef-8e87-969dfdb80e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1) Re‐load just the raw df_final, no added cols ────────────────────────\n",
    "raw = spark.read.parquet(f\"{hadoopFS}/user/com-490/group/U1/raw_df_modelling.parquet\").select(\n",
    "    \"stop_id\",\"operating_day\",\"dow\",\"trip_id\",\"type\",\n",
    "    \"arr_time\",\"arr_actual\",\"dep_time\",\"dep_actual\",\n",
    "    \"arr_delay_sec\",\"total_daily_precip\",\"rained\",\n",
    "    \"stop_name\",\"stop_lat\",\"stop_lon\"\n",
    ")\n",
    "\n",
    "# 2) Cast & basic feature‐engineering\n",
    "df = (\n",
    "    raw\n",
    "    .withColumn(\"arr_delay_sec\",      col(\"arr_delay_sec\"). cast(DoubleType()))\n",
    "    .withColumn(\"total_daily_precip\", col(\"total_daily_precip\").cast(DoubleType()))\n",
    "    .withColumn(\"stop_lat\",           col(\"stop_lat\").          cast(DoubleType()))\n",
    "    .withColumn(\"stop_lon\",           col(\"stop_lon\").          cast(DoubleType()))\n",
    "    .withColumn(\"arr_time_ts\",        to_timestamp(\"arr_time\"))\n",
    "    .na.drop(subset=[\n",
    "        \"arr_delay_sec\",\"total_daily_precip\",\n",
    "        \"stop_lat\",\"stop_lon\",\"arr_time_ts\"\n",
    "    ])\n",
    "    # only Mon–Fri, hours 7–20\n",
    "    .withColumn(\"dow\", col(\"dow\").cast(IntegerType()))\n",
    "    .filter(col(\"dow\").between(1,5))\n",
    "    .withColumn(\"hour\", hour(\"arr_time_ts\"))\n",
    "    .filter(col(\"hour\").between(7,20))\n",
    "    .withColumn(\"is_raining\", when(col(\"total_daily_precip\")>0,1).otherwise(0))\n",
    "    .withColumn(\"dow_str\",\n",
    "        date_format(col(\"operating_day\").cast(DateType()), \"E\")\n",
    "    )\n",
    "    .na.drop()\n",
    ")\n",
    "\n",
    "# 3) Build the shared “features only” pipeline\n",
    "#    (same as before: index dow/type/stop_name → OHE → assemble + cache)\n",
    "dow_idx      = StringIndexer(inputCol=\"dow_str\",   outputCol=\"dow_idx\",  handleInvalid=\"keep\")\n",
    "dow_ohe      = OneHotEncoder(inputCol=\"dow_idx\",    outputCol=\"dow_vec\")\n",
    "type_idx     = StringIndexer(inputCol=\"type\",      outputCol=\"type_idx\", handleInvalid=\"keep\")\n",
    "type_ohe     = OneHotEncoder(inputCol=\"type_idx\",   outputCol=\"type_vec\")\n",
    "stop_idx     = StringIndexer(inputCol=\"stop_name\", outputCol=\"stop_idx\", handleInvalid=\"keep\")\n",
    "stop_ohe     = OneHotEncoder(inputCol=\"stop_idx\",   outputCol=\"stop_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "      \"dow_vec\",\"type_vec\",\"stop_vec\",\n",
    "      \"hour\",\"total_daily_precip\",\"is_raining\",\n",
    "      \"stop_lat\",\"stop_lon\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "feat_pipe  = Pipeline(stages=[\n",
    "    dow_idx, dow_ohe,\n",
    "    type_idx, type_ohe,\n",
    "    stop_idx, stop_ohe,\n",
    "    assembler\n",
    "])\n",
    "\n",
    "feat_model = feat_pipe.fit(df)\n",
    "df_feats   = feat_model.transform(df) \\\n",
    "             .select(\"trip_id\",\"stop_id\",\"features\",\"arr_delay_sec\") \\\n",
    "             .cache()\n",
    "df_feats.count()\n",
    "\n",
    "# 4)  Train a **sign** classifier: `label = 1 if arr_delay_sec≥0 else 0`\n",
    "sign_df = df_feats.withColumn(\n",
    "    \"is_late\", (col(\"arr_delay_sec\") >= 0).cast(IntegerType())\n",
    ")\n",
    "\n",
    "sign_clf = GBTClassifier(\n",
    "    labelCol=\"is_late\", featuresCol=\"features\",\n",
    "    maxIter=20, maxDepth=5, stepSize=0.1, seed=42\n",
    ")\n",
    "\n",
    "sign_model = sign_clf.fit(sign_df)\n",
    "\n",
    "# 5)  Train a single **magnitude** regressor on absolute delay\n",
    "mag_df = df_feats.withColumn(\"abs_delay\", sql_abs(col(\"arr_delay_sec\")))\n",
    "\n",
    "mag_reg = GBTRegressor(\n",
    "    labelCol=\"abs_delay\", featuresCol=\"features\",\n",
    "    maxIter=50, maxDepth=5, stepSize=0.1, seed=42\n",
    ")\n",
    "\n",
    "mag_model = mag_reg.fit(mag_df)\n",
    "\n",
    "# 6)  At inference, score both **once**:\n",
    "scored = sign_model.transform(df_feats) \\\n",
    "          .withColumnRenamed(\"probability\",\"sign_prob\") \\\n",
    "          .withColumnRenamed(\"prediction\",\"sign_pred\")\n",
    "\n",
    "scored = mag_model.transform(scored) \\\n",
    "          .withColumnRenamed(\"prediction\",\"mag_pred\")\n",
    "\n",
    "# 7)  Combine into final signed prediction\n",
    "final = scored.withColumn(\n",
    "    \"final_pred\",\n",
    "    when(col(\"sign_prob\")[1] >= 0.5, col(\"mag_pred\"))\n",
    "   .otherwise(-col(\"mag_pred\"))\n",
    ")\n",
    "\n",
    "# 8)  Inspect\n",
    "final.select(\n",
    "    \"trip_id\",\"stop_id\",\n",
    "    \"arr_delay_sec\",\n",
    "    \"sign_prob\",\"mag_pred\",\"final_pred\"\n",
    ").show(10,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10a143-46cc-4c34-bbd8-700733d8b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_late.dropDuplicates().show(10)\n",
    "pred_early.dropDuplicates().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13227091-e83e-4c14-bedd-e9ceb63892bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α=0.10: empirical coverage ≈ 0.90\n",
      "α=0.50: empirical coverage ≈ 0.50\n",
      "α=0.90: empirical coverage ≈ 0.90\n",
      "   actual        q10        q50         q90\n",
      "0  -676.0 -41.051983  51.063325  138.816753\n",
      "1   153.0 -50.635623  63.343375  309.051633\n",
      "2   -12.0 -16.145387   3.421079   57.356084\n",
      "3   110.0 -54.301806  54.036515  252.750592\n",
      "4    75.0 -65.721357  58.107326  280.471712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1319:>                                                       (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "small_spark = (\n",
    "    raw\n",
    "    .withColumn(\"arr_delay_sec\",       F.col(\"arr_delay_sec\").cast(\"double\"))\n",
    "    .withColumn(\"total_daily_precip\",  F.col(\"total_daily_precip\").cast(\"double\"))\n",
    "    .withColumn(\"hour\",                F.hour(F.to_timestamp(\"arr_time\")).cast(\"int\"))\n",
    "    .withColumn(\"dow_str\",             F.date_format(F.to_timestamp(\"operating_day\"), \"E\"))\n",
    "    .withColumn(\"is_raining\",          F.when(F.col(\"total_daily_precip\") > 0, 1).otherwise(0))\n",
    "    # restrict to Mon–Fri, hours 07–20\n",
    "    .filter(F.col(\"dow_str\").isin(\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\"))\n",
    "    .filter(F.col(\"hour\").between(7,20))\n",
    "    .select(\n",
    "        \"arr_delay_sec\",\n",
    "        \"hour\",\n",
    "        \"total_daily_precip\",\n",
    "        \"is_raining\",\n",
    "        \"dow_str\",\n",
    "        \"type\",\n",
    "        \"stop_name\",\n",
    "        \"stop_lat\",\n",
    "        \"stop_lon\"\n",
    "    )\n",
    "    .sample(False, 0.02, seed=42)   # just 2% sample for speed\n",
    "    .limit(50000)                   # cap at 50 k rows\n",
    ")\n",
    "\n",
    "# --- 2) move into pandas for clustering & interaction features ---\n",
    "pdf = small_spark.toPandas()\n",
    "\n",
    "# interaction terms\n",
    "pdf[\"hour_precip\"] = pdf[\"hour\"] * pdf[\"total_daily_precip\"]\n",
    "pdf[\"hour_rain\"]  = pdf[\"hour\"] * pdf[\"is_raining\"]\n",
    "\n",
    "# cluster stops into 10 “area” buckets\n",
    "from sklearn.cluster import KMeans\n",
    "coords = pdf[[\"stop_lat\",\"stop_lon\"]].astype(float)\n",
    "km = KMeans(n_clusters=10, random_state=42).fit(coords)\n",
    "pdf[\"area\"] = km.labels_.astype(str)\n",
    "\n",
    "# --- 3) split into train/test ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = pdf[\"arr_delay_sec\"].values\n",
    "X = pdf.drop(columns=\"arr_delay_sec\")\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 4) define columns for preprocessing ---\n",
    "cat_cols = [\"dow_str\",\"type\",\"stop_name\",\"area\"]\n",
    "num_cols = [\"hour\",\"total_daily_precip\",\"is_raining\",\n",
    "            \"hour_precip\",\"hour_rain\"]\n",
    "\n",
    "# --- 5) build the sklearn pipeline ---\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.compose        import ColumnTransformer\n",
    "from sklearn.preprocessing  import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble       import GradientBoostingRegressor\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cats\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"nums\", StandardScaler(), num_cols),\n",
    "])\n",
    "\n",
    "# --- 6) train one quantile‐regressor per target quantile ---\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "models = {}\n",
    "for q in quantiles:\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        alpha=q,\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"gbr\", gbr)\n",
    "    ])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    models[q] = pipe\n",
    "\n",
    "# --- 7) assemble out‐of‐sample quantile predictions ---\n",
    "import pandas as pd\n",
    "preds = pd.DataFrame({\"actual\": yte})\n",
    "for q, pipe in models.items():\n",
    "    preds[f\"q{int(q*100)}\"] = pipe.predict(Xte)\n",
    "\n",
    "# quick coverage check\n",
    "for q in quantiles:\n",
    "    col = f\"q{int(q*100)}\"\n",
    "    if q < 0.5:\n",
    "        coverage = (preds[\"actual\"] >= preds[col]).mean()\n",
    "    else:\n",
    "        coverage = (preds[\"actual\"] <= preds[col]).mean()\n",
    "    print(f\"α={q:.2f}: empirical coverage ≈ {coverage:.2f}\")\n",
    "\n",
    "# peek\n",
    "print(preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7b5f1b-5192-4f30-8a2c-a025293e3437",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No feasible route from 8501120 → 8501121 by 2024-01-10 18:10:00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    to_timestamp, unix_timestamp, from_unixtime,\n",
    "    col, lead\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import heapq\n",
    "from datetime import datetime\n",
    "\n",
    "preds2 = (\n",
    "    pred\n",
    "    .filter(col(\"operating_day\") == \"2024-01-10\")    \n",
    "    .withColumn(\"t_dep\",   to_timestamp(\"dep_time\"))  \\\n",
    "    .withColumn(\"delay_s\", col(\"prediction\"))        \\\n",
    "    .filter(col(\"t_dep\").isNotNull() & col(\"delay_s\").isNotNull())\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"trip_id\").orderBy(\"t_dep\")\n",
    "segments = (\n",
    "    preds2\n",
    "    .withColumn(\"next_stop\", lead(\"stop_id\").over(w))\n",
    "    .withColumn(\"t_eff\",\n",
    "        from_unixtime(\n",
    "            unix_timestamp(\"t_dep\") + col(\"delay_s\")\n",
    "        ).cast(\"timestamp\")\n",
    "    )\n",
    "    .filter(col(\"next_stop\").isNotNull())\n",
    "    .select(\"stop_id\",\"next_stop\",\"t_dep\",\"t_eff\",\"prediction\") \n",
    ")\n",
    "\n",
    "edges = segments.collect()\n",
    "graph_inc = {}\n",
    "for eid, r in enumerate(edges):\n",
    "    u       = r[\"stop_id\"]\n",
    "    v       = r[\"next_stop\"]\n",
    "    t_dep   = r[\"t_dep\"]    \n",
    "    t_eff   = r[\"t_eff\"]    \n",
    "    p_on    = 1.0           \n",
    "    graph_inc.setdefault(v, []).append((u, t_dep, t_eff, p_on, eid))\n",
    "\n",
    "\n",
    "def k_latest_routes(graph_inc, source, target, T, K=5):\n",
    "    \"\"\"\n",
    "    Returns up to K tuples of (dep_time, path_edges, confidence)\n",
    "      - dep_time: datetime of departure at `source`\n",
    "      - path_edges: list of (u, v, edge_index) from source→…→target\n",
    "      - confidence: product of per-edge p_on\n",
    "    \"\"\"\n",
    "    # max-heap: store (–dep_ts, node, path_so_far, conf_so_far)\n",
    "    heap = [(-T.timestamp(), target, [], 1.0)]\n",
    "    found = []\n",
    "\n",
    "    while heap and len(found) < K:\n",
    "        neg_ts, v, path, conf = heapq.heappop(heap)\n",
    "        t_v = datetime.fromtimestamp(-neg_ts)\n",
    "\n",
    "        if v == source:\n",
    "            # we have a full path from source→...→target\n",
    "            found.append((t_v, list(reversed(path)), conf))\n",
    "            continue\n",
    "\n",
    "        for u, t_dep, t_eff, p_on, eid in graph_inc.get(v, []):\n",
    "            # can only catch that segment if it gets you in time\n",
    "            if t_eff <= t_v:\n",
    "                new_conf = conf * p_on\n",
    "                new_path = path + [(u, v, eid)]\n",
    "                heapq.heappush(heap, (-t_dep.timestamp(), u, new_path, new_conf))\n",
    "\n",
    "    return found\n",
    "\n",
    "A = \"8501120\"\n",
    "B = \"8501121\"\n",
    "T = datetime(2024, 1, 10, 18, 10)\n",
    "\n",
    "routes = k_latest_routes(graph_inc, A, B, T, K=3)\n",
    "if not routes:\n",
    "    print(f\"No feasible route from {A} → {B} by {T}\")\n",
    "else:\n",
    "    for dep_time, path, conf in routes:\n",
    "        print(f\"\\nDepart {A} @ {dep_time.isoformat()}  (confidence {conf:.2%})\")\n",
    "        for u, v, eid in path:\n",
    "            seg = edges[eid]\n",
    "            print(f\"   {u} → {v}   dep@{seg['t_dep']}   arr_eff@{seg['t_eff']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
