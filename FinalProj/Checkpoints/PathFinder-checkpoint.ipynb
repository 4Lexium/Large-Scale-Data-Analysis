{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4c97cb-dc7b-469a-aee8-e70b34d7077b",
   "metadata": {},
   "source": [
    "### SETUP WORK ENVIROMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4eba4f7-c5b2-4d47-8400-a56548a1bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64 as b64\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"pandas only supports SQLAlchemy connectable .*\")\n",
    "\n",
    "import pwd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from random import randrange\n",
    "import pyspark.sql.functions as F\n",
    "#np.bool = np.bool_\n",
    "\n",
    "import trino\n",
    "from contextlib import closing\n",
    "from urllib.parse import urlparse\n",
    "from trino.dbapi import connect\n",
    "from trino.auth import BasicAuthentication, JWTAuthentication\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from random import randrange\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "groupName='U1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8558432-3c15-4f02-a3f4-f254ea63dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark core functions and types\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, udf, expr, when, explode,\n",
    "    from_unixtime, to_timestamp, unix_timestamp,\n",
    "    year, month, dayofmonth, dayofweek, hour, minute,\n",
    "    radians, sin, cos, atan2, sqrt, pow, exp, abs as abs_,\n",
    "    substring, lower, trim, regexp_replace,\n",
    "    countDistinct, coalesce, lead, lag,\n",
    "    sum as spark_sum, max as spark_max, avg, variance, stddev,\n",
    "    least, greatest, first\n",
    ")\n",
    "from pyspark.sql.types import StringType, DoubleType, ArrayType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Datetime and timezone utilities\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Geospatial and distance calculation\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import shapely.wkb\n",
    "from geopy.distance import geodesic\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Data science utilities\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graph utilities\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "# Widgets and display\n",
    "import ipywidgets as widgets\n",
    "import folium\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f8f95f-d089-474f-b37c-ab25ec27b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsername():\n",
    "    payload = os.environ.get('EPFL_COM490_TOKEN').split('.')[1]\n",
    "    payload=payload+'=' * (4 - len(payload) % 4)\n",
    "    obj = json.loads(b64.urlsafe_b64decode(payload))\n",
    "    if (time.time() > int(obj.get('exp')) - 3600):\n",
    "        raise Exception('Your credentials have expired, please restart your Jupyter Hub server:'\n",
    "                        'File>Hub Control Panel, Stop My Server, Start My Server.')\n",
    "    time_left = int((obj.get('exp') - time.time())/3600)\n",
    "    return obj.get('sub'), time_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4805dd2-e042-40f1-b2c7-67ca836837f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse URL: https://iccluster028.iccluster.epfl.ch:8443/\n",
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "trinoAuth = JWTAuthentication(os.environ.get('EPFL_COM490_TOKEN'))\n",
    "trinoUrl  = urlparse(os.environ.get('TRINO_URL'))\n",
    "Query=[]\n",
    "\n",
    "print(f\"Warehouse URL: {trinoUrl.scheme}://{trinoUrl.hostname}:{trinoUrl.port}/\")\n",
    "\n",
    "conn = connect(\n",
    "    host=trinoUrl.hostname,\n",
    "    port=trinoUrl.port,\n",
    "    auth=trinoAuth,\n",
    "    http_scheme=trinoUrl.scheme,\n",
    "    verify=True\n",
    ")\n",
    "\n",
    "print('Connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c91046b-96f3-4883-ae2c-a6f9de3b25b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\n",
      "hadoopFSs=hdfs://iccluster059.iccluster.epfl.ch:9000\n",
      "username=spasov\n",
      "group=U1\n"
     ]
    }
   ],
   "source": [
    "username = pwd.getpwuid(os.getuid()).pw_name\n",
    "hadoopFS=os.getenv('HADOOP_FS', None)\n",
    "groupName = 'U1'\n",
    "\n",
    "print(os.getenv('SPARK_HOME'))\n",
    "print(f\"hadoopFSs={hadoopFS}\")\n",
    "print(f\"username={username}\")\n",
    "print(f\"group={groupName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40140a33-c558-4782-9e05-c00aa735d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are: spasov\n",
      "credentials validity: 43 hours left.\n",
      "shared namespace is: iceberg.com490_iceberg\n",
      "your namespace is: iceberg.spasov\n",
      "your group is: U1\n"
     ]
    }
   ],
   "source": [
    "username, validity_h = getUsername()\n",
    "hadoopFS = os.environ.get('HADOOP_FS')\n",
    "namespace = 'iceberg.' + username\n",
    "sharedNS = 'iceberg.com490_iceberg'\n",
    "\n",
    "if not re.search('[A-Z][0-9]', groupName):\n",
    "    raise Exception('Invalid group name {groupName}')\n",
    "\n",
    "print(f\"you are: {username}\")\n",
    "print(f\"credentials validity: {validity_h} hours left.\")\n",
    "print(f\"shared namespace is: {sharedNS}\")\n",
    "print(f\"your namespace is: {namespace}\")\n",
    "print(f\"your group is: {groupName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf5285a4-d306-4f60-a2c7-299febfdd0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/29 22:23:35 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(pwd.getpwuid(os.getuid()).pw_name)\\\n",
    "            .config('spark.ui.port', randrange(4040, 4440, 5))\\\n",
    "            .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "            .config('spark.jars', f'{hadoopFS}/data/com-490/jars/iceberg-spark-runtime-3.5_2.13-1.6.1.jar')\\\n",
    "            .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    "            .config('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog')\\\n",
    "            .config('spark.sql.catalog.iceberg.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.iceberg.warehouse', f'{hadoopFS}/data/com-490/iceberg/')\\\n",
    "            .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.type', 'hadoop')\\\n",
    "            .config('spark.sql.catalog.spark_catalog.warehouse', f'{hadoopFS}/user/{username}/assignment-3/warehouse')\\\n",
    "            .config(\"spark.sql.warehouse.dir\", f'{hadoopFS}/user/{username}/assignment-3/spark/warehouse')\\\n",
    "            .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'G1 Young Generation')\\\n",
    "            .config(\"spark.executor.memory\", \"6g\")\\\n",
    "            .config(\"spark.executor.cores\", \"4\")\\\n",
    "            .config(\"spark.executor.instances\", \"4\")\\\n",
    "            .master('yarn')\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e40c1dd-06ea-4973-a11a-7ebe35501a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://iccluster087.iccluster.epfl.ch:4295\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spasov</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=spasov>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef8e83-c5d2-42f7-9fd0-9ce4ac264b36",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835ff5e6-3e54-409b-943d-8f2aa13e948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported successfully!\n"
     ]
    }
   ],
   "source": [
    "#IMPORT custom scripts file\n",
    "\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "# Define path to the file\n",
    "file_path = os.path.abspath('./scripts/pathfinder.py')\n",
    "# Load the module\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", file_path)\n",
    "pathfinder = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(pathfinder)\n",
    "print(pathfinder.test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca52906-55b3-4b1e-b064-676e25629161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geopy\n",
    "# !pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c6e02-6e31-4ce6-9472-3df283dcfb97",
   "metadata": {},
   "source": [
    "### IMPORT FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71410d70-da1a-4598-97f2-c26ec281bf92",
   "metadata": {},
   "source": [
    "### PREPARE SPARK DATAFRAMES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b05e6d0-e740-4ce7-9063-652c0181657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# model = spark.read.parquet(f\"{hadoopFS}/user/com-490/group/U1/multiclass_model.parquet\")\n",
    "master = spark.read.parquet(f\"{hadoopFS}/user/com-490/group/U1/edges\")\n",
    "master = master.drop(\"route_id\")\n",
    "model = pathfinder.get_confidence_df(spark)\n",
    "# master.show()\n",
    "# model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ab796a6-787a-400b-bf2e-b174d2905c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# master.filter(col(\"from_stop\") == '8595939') \\\n",
    "#     .filter(col(\"to_stop\") == '8595937') \\\n",
    "#     .filter(col(\"dow\") == 3) \\\n",
    "#     .filter(col(\"t_arrival\") >= '13:00:00') \\\n",
    "#     .orderBy(\"t_arrival\") \\\n",
    "#     .show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14350ec4-573d-46c6-bffa-c670dda449d0",
   "metadata": {},
   "source": [
    "### USER TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c29da30f-521b-41c6-bbdb-9554ebc97ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6416008fe7434eacf906831594249f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combobox(value='Lausanne, Bellerive', continuous_update=False, description='Origin:', ensure_option=True, layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a38f57f621643b99bceb25f6dff95c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combobox(value='Ecublens VD, EPFL (bus)', continuous_update=False, description='Target:', ensure_option=True, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0a76967e114654b2617aee00b71680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='15:00:00', description='Arrival Time:', layout=Layout(width='300px'), placeholder='HH:MM:SS')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e23515c5ce4ffdb15ce12f8a6db159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Day of Week:', index=3, layout=Layout(width='300px'), options={'Monday': 0, 'Tuesday': 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e114cb9581f4d5eb3c79a4a8bc301c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a965d9a0a60479f884bb421ed8e447f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "existing_stops = pathfinder.get_all_close_stop_pairs(spark, radius_meters=500)[1]\n",
    "# Create stop dictionary\n",
    "stop_name_dict = {\n",
    "    row[\"stop_id\"]: row[\"stop_name\"]\n",
    "    for row in existing_stops.select(\"stop_id\", \"stop_name\").collect()\n",
    "}\n",
    "\n",
    "submit_button = pathfinder.create_stop_selector_widget(spark, master, model, stop_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e35180-c9e2-4d99-8a97-752bded27ecd",
   "metadata": {},
   "source": [
    "**Comment:** An obvious inconsistency with the result is that the model predicts very small (or no confidence) in making successful trips. This is attributed to inconsistencies in the timetable data where the same trip is assigned multiple trip_ids, or multiple trips share the same trip_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9993b-0421-4136-9ffc-62c356d8808c",
   "metadata": {},
   "source": [
    "# PATH FINDING ALGORITHM\n",
    "\n",
    "The goal is to find the shortest and less risky path from given origin to a target and target arrival time.\n",
    "\n",
    "Dijkstra algorithm computes the shortest path from a given origin (A) to ***all*** other acessible points (from A) within a defined network. A connection between nodes A and B is called an edge.\n",
    "The method checks and stores distances to all nodes in vicinty () of the current node. Then moves to the closest node that has not been visited yet. Provided the network is *well-connected* meaning: no isolated stops and no connections leading far far away from the city and the stop clustering. This is for sure not going to be a primary issue. The method explores the distance (edge weight) to all nodes and does so with compexity $\\mathcal{O}((V + E) \\log V)$, where $V$ is number of network nodes (stops) and $E$ is number of edges (connections). This is the worst case scenario, with luck and good origin the method completes much faster.  \n",
    "\n",
    "The standard netowrxx library function offers a well optimized method with option to generate the fastest path from the origin to the stop you desire. \n",
    "Going further, using the **Yen's algorithm**, which is sucessive to Dijstra alg. You go throught Dijsktra path and then find the best node to branch off at and recompute a so-called Spur path by the same principles. This gives you two (and in principle *k* best paths using networxx shortest_distance).\n",
    "\n",
    "The issue are the Edge weights! In our code an edge A and B will carry 3 parameters, be it walking or vehicle between: **T_nominal**, **t_arrival** @B and **R_risk** @A. T_nominal is the scheduled travel time comming from trip schedule (*master*) or geospatial distance divided by walking speed. R_risk is the expected dalay that we get from (*model*). It goes by stopID, day of week and hour of day. Ideally we wanted to join by type of transport but the table master comes from has a very weird style for transporation labeling, which made a join impossible.\n",
    "\n",
    "Now, Dijkstra algorithm is *static* is the sense that the edgeweight dont reflect time propagation. It is therefore an excellent method for path finding based on distance which is static. \n",
    "As an example:\n",
    "\tYou want tot go to EPFL from Ouchy. You took the 24 buss to Bourdonnette at 12.00 and now want to go futher by m1. Dijkstra checked multiple options 12.15, 12.35, 12.55, 13.15 etc (maybe even 11.35 cause why not) and came to the conclusion that based on delays R and Tnominal alone, the 11.35 or 12.35 is fastest (has less delays or fastest nominal travel). Static Dijkstra is clueless about waiting or even what causality is!!!\n",
    "\n",
    "To combat this we use t_arrival vs CLK (clock). We tried implementing our own algorithm modification where the propagation would consider both weights and time. You can run the **dynamic_dijkstra** and see the result. At this stage the path is a collection of many many small steps (that seem right, but in the grand total dont leed toward the target). Add the fact that there are many tripID issues in master with trips missing/disappearing mid route (more on that later) causes the path to become a *\"browinaian motion\"* with some sanity restrictions. To highlight the two main difficulties: Find a different way to trace vehicle edges (with consistent schedules), **Directionaity** -we walk all over the place without nearing the target.\n",
    "\n",
    "*The first problem can be seen by running a simple filtering like so: \n",
    "\n",
    "master.filter(col(\"from_stop\") == '8595939') \\\n",
    "    .filter(col(\"to_stop\") == '8595937') \\\n",
    "    .filter(col(\"dow\") == 3) \\\n",
    "    .filter(col(\"t_arrival\") >= '13:00:00') \\\n",
    "    .orderBy(\"t_arrival\") \\\n",
    "    .show(30, truncate=False)\n",
    "\n",
    "If done for stops pairs that you know follow each other, you can trace how some tripIDs disappear mid route or simply dont exist at all. In one hour window a pair will have 6 different instances (counting two directions), while the next hour its only 4, or 0! This inconsitency makes reliable back-tracing impossible, and the reason in the output you see so many walking edges. \n",
    "\n",
    "*The second problem with lack of directionality is **solved** by using static dijkstra to determine the optimal path based on R and T alone. It is navigating a geographical \"heatmap\" of delays and the scheduled travel. Even though the algorithm still doesnt consider causality and wating, the stops sequence it deems optimal are likely to be best verion (and most importantly we can have a garantee that both origin and target are found!). \n",
    "The example I mentioned earlier is actually safe in this approach. You recover the important Ouchy... -> Bourdonette -> EPFL, and can then **backtrace** but with direct directives as to where! \n",
    "\n",
    "After the **skeleton** is computed, we can backtrace and select best tripIDs based on *t_arrival* and *CLK*. All would be great, if it werent for tripID issue in transit_edges, and sometimes missing walking edges. Walking edges missing is due to the max distance requirement, but tripID iconsitencies is an actuall plague. Keep in mind we apply an overall time-window by default 1 hour, so if its a long journey that has to wait a lot (because of missing trips) the will end up going out of time bounds before raching origin in backwards propagation.\n",
    "\n",
    "Finally the risk assessment. At this point, the risk is more a measure of likelyhood of inconsitency in transit edges, than acutally what it is supposed to mean. Indeed using the formula $((wait-walk)-R\\_risk/std\\_dev(R)$ looses meaning if either the walking or transit edges is None. Which is what happens in many cases. If something is missing, we have to use default values, but the logic of the formula is still lost, and with it the siginificance of the overall cumulative confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472f242-48c7-4b21-808d-ba2c6b30c7b2",
   "metadata": {},
   "source": [
    "### ALTERNATIVE TO WIDGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82dc4007-366a-42a8-8349-11b17655b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE TO WIDGET\n",
    "\n",
    "# # First for widget to get correct options\n",
    "# existing_stops = get_all_close_stop_pairs(spark, radius_meters=500)[1]\n",
    "# # Create stop dictionary\n",
    "# stop_name_dict = {\n",
    "#     row[\"stop_id\"]: row[\"stop_name\"]\n",
    "#     for row in existing_stops.select(\"stop_id\", \"stop_name\").collect()\n",
    "# }\n",
    "\n",
    "# # Inputs for test\n",
    "# # 8595937 UNIL sport\n",
    "# # 8591989 Lausanne, Bellerive\n",
    "# # 8501214 Ecoublens VD, EPFL\n",
    "# # 8501210 Lausanne, Bourdonnette\n",
    "# # 8501209 Lausanne, MAlley\n",
    "\n",
    "# # If you want your own:\n",
    "# # name_to_id = {v: k for k, v in stop_name_dict.items()}\n",
    "# # stop_id = name_to_id[\"Lausanne, Vidy-Port\"]\n",
    "# # print(stop_id)\n",
    "\n",
    "# origin, target = \"8591989\", \"8595937\"  \n",
    "# arrival = \"15:00:00\"\n",
    "# dow = \"3\"\n",
    "# time_window_sec=5400\n",
    "\n",
    "# # Create the edges and nodes\n",
    "# existing_stops, transit_edges, walking_edges = get_data(master, model, arrival, dow, time_window_sec)\n",
    "\n",
    "# # transit_edges.show()\n",
    "# # walking_edges.show()\n",
    "\n",
    "# # Build the Network\n",
    "# graph = build_graph_from_edges(transit_edges, walking_edges, risk=True)\n",
    "\n",
    "# # Run your static method on the network\n",
    "# # path = dijkstra_path(graph, origin, target)\n",
    "# path_prim, path_spur = multi_path(graph, origin, target)\n",
    "\n",
    "# # Visulatize the path, static skeleton path.\n",
    "# print(path_prim)\n",
    "# print(path_spur)\n",
    "# plot_stop_network_with_path(graph, existing_stops, path_prim, path_spur)\n",
    "\n",
    "\n",
    "# path_rendering, danger = dynamic_backtracing(path_prim, arrival, transit_edges, walking_edges)\n",
    "# path_rendering_df = display_path(path_rendering, stop_name_dict)\n",
    "# print(f\"Cumulative Risk Factor: {math.exp(danger)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1caf24-fae3-4f14-b4f3-f3c3f61e7130",
   "metadata": {},
   "source": [
    "### TIME DEPENDANT DIJKSTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2ab5c-3728-4b8c-b536-7f28a0ebd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin, target = \"8591989\", \"8595937\"\n",
    "# arrival = \"15:00:00\"\n",
    "# dow = \"3\"\n",
    "# time_window_sec=3600\n",
    "\n",
    "# existing_stops, transit_edges, walking_edges = pathfinder.get_data(master, model, arrival, dow, time_window_sec)\n",
    "# graph = pathfinder.get_network_dict(existing_stops, transit_edges, walking_edges)\n",
    "# path_rendering, map_rendering, visited = pathfinder.dynamic_dijkstra(graph, origin, target, arrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "97a822bf-b7d3-43ad-aad6-5cbe646196da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
